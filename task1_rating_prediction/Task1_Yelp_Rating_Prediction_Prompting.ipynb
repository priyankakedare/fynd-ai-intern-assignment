{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f059a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Priyanka\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84575be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load API Key\n",
    "llm = ChatGroq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name=\"llama-3.1-8b-instant\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080b12d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "6252  We got here around midnight last Friday... the...      4\n",
       "4684  Brought a friend from Louisiana here.  She say...      5\n",
       "1731  Every friday, my dad and I eat here. We order ...      3\n",
       "4742  My husband and I were really, really disappoin...      1\n",
       "4521  Love this place!  Was in phoenix 3 weeks for w...      5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Yelp Dataset\n",
    "df = pd.read_csv(\"yelp.csv\")\n",
    "\n",
    "df = df[['text', 'stars']].dropna()\n",
    "df = df.sample(200, random_state=42)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe1fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'stars'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe04efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Prompt Version 1 (Basic)\n",
    "def prompt_v1(review):\n",
    "    return f\"\"\"\n",
    "    Read the following Yelp review and predict a star rating from 1 to 5.\n",
    "\n",
    "    Return ONLY valid JSON in this format:\n",
    "    {{\n",
    "      \"predicted_stars\": number,\n",
    "      \"explanation\": \"short reason\"\n",
    "    }}\n",
    "\n",
    "    Review:\n",
    "    {review}\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a797ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [08:32<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "#Run Prompt V1 on Data\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "results_v1 = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    review = row['text']\n",
    "    actual = row['stars']\n",
    "\n",
    "    response = llm.invoke(prompt_v1(review)).content\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        predicted = parsed[\"predicted_stars\"]\n",
    "        valid_json = True\n",
    "    except:\n",
    "        predicted = None\n",
    "        valid_json = False\n",
    "\n",
    "    results_v1.append({\n",
    "        \"actual\": actual,\n",
    "        \"predicted\": predicted,\n",
    "        \"json_valid\": valid_json\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96781c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.665), np.float64(1.0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate Prompt V1\n",
    "res1 = pd.DataFrame(results_v1)\n",
    "\n",
    "accuracy_v1 = (res1[\"actual\"] == res1[\"predicted\"]).mean()\n",
    "json_rate_v1 = res1[\"json_valid\"].mean()\n",
    "\n",
    "accuracy_v1, json_rate_v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974d07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Version 2 (Improved)\n",
    "def prompt_v2(review):\n",
    "    return f\"\"\"\n",
    "    You are a professional Yelp review analyst.\n",
    "\n",
    "    Your task is to assign a star rating from 1 to 5 based strictly on:\n",
    "    - Overall sentiment\n",
    "    - Strength of language\n",
    "    - Positive vs negative aspects\n",
    "\n",
    "    Be conservative (avoid extreme ratings unless clearly justified).\n",
    "    Respond ONLY with valid JSON. No extra text.\n",
    "\n",
    "    Review:\n",
    "    \"{review}\"\n",
    "\n",
    "    JSON format:\n",
    "    {{\n",
    "      \"predicted_stars\": number,\n",
    "      \"explanation\": \"short justification\"\n",
    "    }}\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c422d3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [10:09<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "results_v2 = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    review = row['text']\n",
    "    actual = row['stars']\n",
    "\n",
    "    response = llm.invoke(prompt_v2(review)).content\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        predicted = parsed[\"predicted_stars\"]\n",
    "        valid_json = True\n",
    "    except:\n",
    "        predicted = None\n",
    "        valid_json = False\n",
    "\n",
    "    results_v2.append({\n",
    "        \"actual\": actual,\n",
    "        \"predicted\": predicted,\n",
    "        \"json_valid\": valid_json\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102cb0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.69), np.float64(1.0))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = pd.DataFrame(results_v2)\n",
    "\n",
    "accuracy_v2 = (res2[\"actual\"] == res2[\"predicted\"]).mean()\n",
    "json_rate_v2 = res2[\"json_valid\"].mean()\n",
    "\n",
    "accuracy_v2, json_rate_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe5b4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Version 3 (Best Prompt)\n",
    "def prompt_v3(review):\n",
    "    return f\"\"\"\n",
    "    You are a Yelp review rating expert.\n",
    "\n",
    "    Example 1:\n",
    "    Review: \"Amazing food, friendly staff, will come again!\"\n",
    "    Output: {{ \"predicted_stars\": 5, \"explanation\": \"Very positive experience\" }}\n",
    "\n",
    "    Example 2:\n",
    "    Review: \"Food was okay but service was slow and rude.\"\n",
    "    Output: {{ \"predicted_stars\": 3, \"explanation\": \"Mixed experience\" }}\n",
    "\n",
    "    Now analyze the review below and predict the rating.\n",
    "\n",
    "    Review:\n",
    "    \"{review}\"\n",
    "\n",
    "    Respond ONLY with valid JSON:\n",
    "    {{\n",
    "      \"predicted_stars\": number,\n",
    "      \"explanation\": \"brief reason\"\n",
    "    }}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53d6afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [10:57<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "#Final Comparison Table\n",
    "results_v3 = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    review = row['text']\n",
    "    actual = row['stars']\n",
    "\n",
    "    response = llm.invoke(prompt_v3(review)).content\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        predicted = parsed[\"predicted_stars\"]\n",
    "        valid_json = True\n",
    "    except:\n",
    "        predicted = None\n",
    "        valid_json = False\n",
    "\n",
    "    results_v3.append({\n",
    "        \"actual\": actual,\n",
    "        \"predicted\": predicted,\n",
    "        \"json_valid\": valid_json\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49758b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.64), np.float64(1.0))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3 = pd.DataFrame(results_v3)\n",
    "\n",
    "accuracy_v3 = (res3[\"actual\"] == res3[\"predicted\"]).mean()\n",
    "json_rate_v3 = res3[\"json_valid\"].mean()\n",
    "\n",
    "accuracy_v3, json_rate_v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973608da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>JSON Validity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1 Basic</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V2 Strict</td>\n",
       "      <td>0.690</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V3 Few-shot</td>\n",
       "      <td>0.640</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prompt  Accuracy  JSON Validity\n",
       "0     V1 Basic     0.665            1.0\n",
       "1    V2 Strict     0.690            1.0\n",
       "2  V3 Few-shot     0.640            1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final Comparison Table\n",
    "comparison = pd.DataFrame({\n",
    "    \"Prompt\": [\"V1 Basic\", \"V2 Strict\", \"V3 Few-shot\"],\n",
    "    \"Accuracy\": [accuracy_v1, accuracy_v2, accuracy_v3],\n",
    "    \"JSON Validity\": [json_rate_v1, json_rate_v2, json_rate_v3]\n",
    "})\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647c55d",
   "metadata": {},
   "source": [
    "# Task 1 – Yelp Rating Prediction via Prompt Engineering\n",
    "\n",
    "This notebook evaluates three different prompting strategies for predicting Yelp star ratings using an LLM.  \n",
    "We compare accuracy and JSON validity across prompts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
